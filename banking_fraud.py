# -*- coding: utf-8 -*-
"""Banking_fraud.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wyUZ7HgpcLp_exxc6oUGO3XJ5s252r_S
"""



from google.colab import drive
drive.mount('/gdrive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re

path = '/gdrive/My Drive/Datasets/'
train = pd.read_csv(path + 'JH #8 Train.csv')
train.head()

IDcol = train['Loan_ID'].astype('int32').dtypes

IDcol.type

train = train.drop(columns= ['Loan_ID'])
train.head()

train.isnull().sum()

bftype = train.dtypes
bftype

train.count()

train['Annual_Income'].fillna((train['Annual_Income'].mean()), inplace=True)
train.head()

train['Months_Since_Deliquency'].fillna((train['Months_Since_Deliquency'].median()), inplace=True)
train['Length_Employed'].fillna(train['Length_Employed'].mode()[0], inplace=True)
train['Home_Owner'].fillna(train['Home_Owner'].mode()[0], inplace=True)
train.head()

train.isnull().sum()



train.dropna(inplace = True)
train.head()

train.count()

train['Length_Employed'] = train['Length_Employed'].astype('category')
aft = train.dtypes
aft

from sklearn.preprocessing import LabelEncoder

lee = LabelEncoder()
train['Length_Employed'] = lee.fit_transform(train['Length_Employed'])

hoe = LabelEncoder()
train['Home_Owner'] = hoe.fit_transform(train['Home_Owner'])

ive = LabelEncoder()
train['Income_Verified'] = ive.fit_transform(train['Income_Verified'])

pole = LabelEncoder()
train['Purpose_Of_Loan'] = pole.fit_transform(train['Purpose_Of_Loan'])

ge = LabelEncoder()
train['Gender'] = ge.fit_transform(train['Gender'])

lare = LabelEncoder()
train['Loan_Amount_Requested'] = lare.fit_transform(train['Loan_Amount_Requested'])

#target = pd.to_numeric(train['Interest_Rate'], errors = 'coerce')
target = train['Interest_Rate']
#aft = train.dtypes
#aft
train = train.drop(columns = ['Interest_Rate'])

train.head()

#train['Loan_Amount_Requested'] = pd.to_numeric(train['Loan_Amount_Requested'], errors='coerce')

a = train.dtypes
a

# Commented out IPython magic to ensure Python compatibility.
from xgboost.sklearn import XGBClassifier
from sklearn import model_selection, metrics   #Additional scklearn functions
from sklearn.model_selection import GridSearchCV   #Perforing grid search

import matplotlib.pylab as plt
# %matplotlib inline
from matplotlib.pylab import rcParams
rcParams['figure.figsize'] = 12, 4

def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):
    
    if useTrainCV:
        xgb_param = alg.get_xgb_params()
        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)
        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds, metrics='auc', early_stopping_rounds=early_stopping_rounds)
        alg.set_params(n_estimators=cvresult.shape[0])
    
    #Fit the algorithm on the data
    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')
        
    #Predict training set:
    dtrain_predictions = alg.predict(dtrain[predictors])
    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]
        
    #Print model report:
    print ("\nModel Report")
    print ("Accuracy : %.4g" % metrics.accuracy_score(dtrain[target].values, dtrain_predictions))
    print ("AUC Score (Train): %f" % metrics.roc_auc_score(dtrain[target], dtrain_predprob))
                    
    feat_imp = pd.Series(alg.booster().get_fscore())
    feat_imp.plot(kind='bar', title='Feature Importances')
    plt.ylabel('Feature Importance Score')

'''predictors = [x for x in train.columns if x not in [target, IDcol]]
xgb1 = XGBClassifier(
 learning_rate =0.1,
 n_estimators=1000,
 max_depth=5,
 min_child_weight=1,
 gamma=0,
 subsample=0.8,
 colsample_bytree=0.8,
 objective= 'binary:logistic',
 nthread=4,
 scale_pos_weight=1,
 seed=27)
modelfit(xgb1, train, predictors)'''

from sklearn.ensemble import RandomForestClassifier

model_rf = RandomForestClassifier(oob_score = True, max_features = 'auto', n_estimators = 100, min_samples_leaf = 2, random_state = 2)

pd.isnull(train).sum() > 0

model_rf.fit(train, target)

yp = model_rf.predict(train)

from sklearn.metrics import accuracy_score
acc = accuracy_score(target, yp)
print(acc)

train.isnull().sum().sum()

import matplotlib.pyplot as plt

from sklearn.datasets import make_gaussian_quantiles
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier

'''X_train = train
y_train = target
bdt_real = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=2),
    n_estimators=600,
    learning_rate=1)

bdt_discrete = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=2),
    n_estimators=600,
    learning_rate=1.5,
    algorithm="SAMME")

bdt_real.fit(X_train, y_train)
bdt_discrete.fit(X_train, y_train)

real_test_errors = []
discrete_test_errors = []'''

'''for real_test_predict, discrete_train_predict in zip(
        bdt_real.staged_predict(X_test), bdt_discrete.staged_predict(X_test)):
    real_test_errors.append(
        1. - accuracy_score(real_test_predict, y_test))
    discrete_test_errors.append(
        1. - accuracy_score(discrete_train_predict, y_test))

n_trees_discrete = len(bdt_discrete)
n_trees_real = len(bdt_real)

# Boosting might terminate early, but the following arrays are always
# n_estimators long. We crop them to the actual number of trees here:
discrete_estimator_errors = bdt_discrete.estimator_errors_[:n_trees_discrete]
real_estimator_errors = bdt_real.estimator_errors_[:n_trees_real]
discrete_estimator_weights = bdt_discrete.estimator_weights_[:n_trees_discrete]

plt.figure(figsize=(15, 5))

plt.subplot(131)
plt.plot(range(1, n_trees_discrete + 1),
         discrete_test_errors, c='black', label='SAMME')
plt.plot(range(1, n_trees_real + 1),
         real_test_errors, c='black',
         linestyle='dashed', label='SAMME.R')
plt.legend()
plt.ylim(0.18, 0.62)
plt.ylabel('Test Error')
plt.xlabel('Number of Trees')

plt.subplot(132)
plt.plot(range(1, n_trees_discrete + 1), discrete_estimator_errors,
         "b", label='SAMME', alpha=.5)
plt.plot(range(1, n_trees_real + 1), real_estimator_errors,
         "r", label='SAMME.R', alpha=.5)
plt.legend()
plt.ylabel('Error')
plt.xlabel('Number of Trees')
plt.ylim((.2,
         max(real_estimator_errors.max(),
             discrete_estimator_errors.max()) * 1.2))
plt.xlim((-20, len(bdt_discrete) + 20))

plt.subplot(133)
plt.plot(range(1, n_trees_discrete + 1), discrete_estimator_weights,
         "b", label='SAMME')
plt.legend()
plt.ylabel('Weight')
plt.xlabel('Number of Trees')
plt.ylim((0, discrete_estimator_weights.max() * 1.2))
plt.xlim((-20, n_trees_discrete + 20))

# prevent overlapping y-axis labels
plt.subplots_adjust(wspace=0.25)
plt.show()'''

test = pd.read_csv(path + 'JH #8 Test.csv')
test.head()

test = test.drop(columns= ['Loan_ID'])
test.head()

test.isnull().sum()

test['Annual_Income'].fillna((test['Annual_Income'].mean()), inplace=True)
test.head()
test['Months_Since_Deliquency'].fillna((test['Months_Since_Deliquency'].median()), inplace=True)
test['Length_Employed'].fillna(test['Length_Employed'].mode()[0], inplace=True)
test['Home_Owner'].fillna(test['Home_Owner'].mode()[0], inplace=True)
test.head()

test.dropna(inplace = True)
test.head()

test.isnull().sum()

test['Length_Employed'] = test['Length_Employed'].astype('category')
aft = test.dtypes
aft

test['Length_Employed'] = lee.fit_transform(test['Length_Employed'])

test['Home_Owner'] = hoe.fit_transform(test['Home_Owner'])

test['Income_Verified'] = ive.fit_transform(test['Income_Verified'])

test['Purpose_Of_Loan'] = pole.fit_transform(test['Purpose_Of_Loan'])

test['Gender'] = ge.fit_transform(test['Gender'])

test['Loan_Amount_Requested'] = lare.fit_transform(test['Loan_Amount_Requested'])

test.head()

pred = model_rf.predict(test)

pp = pd.DataFrame(pred)

pp.to_csv('RFJH.csv', index = False)

estimator = model_rf.estimators_[5]

from sklearn.tree import export_graphviz

export_graphviz(estimator, out_file='tree.dot', 
                feature_names = iris.feature_names,
                class_names = iris.target_names,
                rounded = True, proportion = False, 
                precision = 2, filled = True)


from subprocess import call
call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])